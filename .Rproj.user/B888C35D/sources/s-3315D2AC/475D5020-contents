---
title: "BM_hw5_jg4197"
author: "Jin Ge"
date: "12/3/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)

library(tidyverse)
library(arsenal)
library(patchwork)
library(faraway)
library(MASS)
library(caret)
library("leaps")
theme_set(theme_minimal())
```

_a) provide desriptive statistics for all variables of interest (continuous and categorical) - no test required._
```{r problem_1a)}
## load and read the data
state_df <- as.data.frame(state.x77) %>% 
  janitor::clean_names() 

## descriptive statistics
control_tbl <- tableby.control(
              test = F,
              total = F,
              numeric.stat = c("meansd", "medianq1q3", "iqr"),
              digits = 2,
              stat.labels = list(
                meansd = "Mean (SD)",
                medianq1q3 = "Median (Q1, Q3)",
                iqr = "IQR"))

change_df <- state_df %>% 
  dplyr::select(life_expectancy = life_exp, HS_grad = hs_grad, everything())

state_tbl <- tableby(~ life_expectancy + population + income + illiteracy + murder + HS_grad + frost + area, data = change_df, control = control_tbl)
summary(state_tbl, title = "Descriptive Statistics: Census Bureau in 50 States of US",
        text = T)
```

_b) Examine exploratory plots, e.g., scatter plots, histograms, box-plots to get a sense of the data and possible variable transformations._
```{r problem_1b)}
## histogram
hist_p <- state_df %>% 
  pivot_longer(population:area,
               names_to = "predictor",
               values_to = "value") %>% 
  ggplot(aes(x = value, y =..density..)) + geom_histogram(aes(fill = predictor)) + 
  geom_density(alpha = .3, aes(fill = predictor)) +
  facet_wrap(~predictor, scales = "free", nrow = 4, ncol = 2) +
  labs(title = "Histogram with density plots by variables",
       fill = "Predictor/Outcome") +
  scale_fill_discrete(labels = c("Area", "Frost", "HS_Grad", "Illteracy", "Income",
                                 "Life expectancy", "Murder", "Population"))
hist_p

## boxplot
box_p <- state_df %>% 
  pivot_longer(population:area,
               names_to = "predictor",
               values_to = "value") %>% 
  ggplot(aes(x = predictor, y = value, color = predictor)) +
  geom_boxplot() + facet_wrap(~predictor, scales = "free", nrow = 3, ncol = 3) +
  labs(title = "Boxplot by variables",
       color = "Predictor/Outcome") +
  scale_color_discrete(labels = c("Area", "Frost", "HS_Grad", "Illteracy", "Income",
                                 "Life expectancy", "Murder", "Population"))
box_p

## test the assumption of linear model
reg_ml <- lm(life_exp~., data = state_df)
par(mfrow = c(2,2))
plot(reg_ml)

## test the Alaska's covariate value
### Alaska has population 365 which is a heavy outlier of population, therefore, Alaska is the influential point and also an outlier in population.
```

Description: Main outcome is ‘Life expectancy’(life_exp). 
Main predictors/covariates that are potentially associated with the outcome: the model should be adjusted for some factors, including ‘Area’, 'Income', 'Illiteracy', ‘Murder’, ‘HS Grad’, ‘Frost’, and 'Population'. 
According to the histogram, 'Life expectancy' is slightly left-skewed. 
'Population' has a heavily right-skewed pattern. 
'Area' has a heavily right-skewed distribution pattern with potential outlier at the right side. 
'Income' is slightly right-skewed. 
Therefore, we choose median and IQR to describe variables, such as 'Life expectancy', 'Population', 'Income' and 'Area'. The left variables are sort of left-skewed, except 'HS Grad' which is roughly symmetric. Then we also use median and IQR to describe these left-skewed variables. In addtion,  we can use mean for 'HS Grad' as its descriptive statistic.

Whether to do transformation: According to histogram, Y(outcome) approximately follows a normal distribution pattern with slight left-skewness. Also, from the QQplot, what we can conclude that: 
residuals are approximately normally distributed because those points fit roughly the straight line (with some points distributing equally around the line), and there are moderate heavy tails (indeed some deviations on both tails). 
Moreover, referring to the Residuals vs Fitted Values plot, residuals do bounce around 0 which is roughly a random pattern. In addition, Scale-Location plot shows that the residuals spread equally along the range of predictors and we can see a roughly horizontal line with equally spread points (random pattern), thus the assumption of equal variance has been followed. 
Last but not least, reading from the Residuals vs Leverage plot, we do see an outlying value (observation #2) at the upper or lower right corner (cases outside of a dashed line, Cook’s distance). 

_c) Use automatic procedures to find a ‘best subset’ of the full model. Present the results and comment on the following: 1) Do the procedures generate the same model?_
```{r problem_1c)}
## backward elimination beginning with the full model
# using step function which uses AIC criterion
step(reg_ml, direction = 'backward')
## popultion +  murder + hs_grad + frost


## forward elimination beginning with non-predictor model
step(lm(life_exp ~ 1., data = state_df), direction = 'forward', 
     trace = 1, scope = formula(reg_ml))
## murder + hs_grad + frost + population


## stepwise automatic procedure
step(reg_ml, direction = 'both')
## population + murder + hs_grad + frost

## construct the selected model
auto_ml <- lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)
summary(auto_ml)
```

The answer is yes. the backward and forward procedures generate the same model with a subset of 4 predictors. Moreover, stepwise automatic procedure gives the same model as above which is: life_exp ~ population + murder + hs_grad + frost.

_c) 2) Is there any variable a close call? What was your decision: keep or discard? Provide arguments for your choice._

According to backward and forward elimination procedures, "population" has a p-value which is 0.052 close to 'failing to reject' if we choose alpha to be 0.05. However, considering what we want to do is to examine the association between predictors and outcome, we hope to leverage the critical value to be more inclusive or tolerant, and less stringent in the model selection. So we better have *alpha-to-remove equals to 0.15 or 0.20*. Therefore, I think keeping this variable in the model is a better choice.


_c) 3) Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’ contain both?_
```{r problem_1c3)}
## correlation matrix
round(cor(state_df),3)
pairs(state_df)

vif_lm <- lm(illiteracy ~ population + murder + hs_grad + frost, data = state_df)
vif(vif_lm)
```

Yes, there is an association/correlation/collinearity between ‘Illiteracy’ and ‘HS graduation rate’. According to my correlation matrix since the value between two variables is equal to -0.657 which is kind of high. No, my ‘subset’ does not contain both variable, it only contains 'HS graduation rate'.

_d) Use criterion-based procedures studied in class to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical)_
```{r problem_1d)}
## construct a function for model selection
best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  

round(best(auto_ml, nbest = 1), 3)

b <- regsubsets(life_exp~., data = state_df)
   (rs <- summary(b))

par(mar = c(4,4,1,1))
par(mfrow = c(1,2))

plot(2:8, rs$cp, xlab = "No of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:8, rs$adjr2, xlab = "No of parameters", ylab="Adj R2")
```


_e) Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. Using this ‘final’ model do the following: 1) Identify any leverage and/or influential points and take appropriate measures.2) Check the model assumptions._
```{r problem_1e)}
my_ml = lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)

stu_res <- rstandard(my_ml)
outliers_y <- stu_res[abs(stu_res) > 2.5]
outliers_y

## give DFFITS, Cook's distance, H_hat matrix
influence.measures(reg_my_choice)

par(mfrow = c(2,2))
plot(my_ml)
```

_e)3) Test the model predictive ability using a 10-fold cross-validation (10 repeats)._
```{r problem_1e3)}
# Use 10-fold validation and create the training sets

data_train <- trainControl(method = "cv", number = 10)

# Fit the 4-variables model
model_caret <- train(life_exp ~ population + murder + hs_grad + frost, 
                   data = P2,
                   trControl = data_train,
                   method = 'lm',
                   na.action = na.pass)
  
# Model predictions using 9 parts of the data fro training 
model_caret

#Rsquared - good % variantion acconted for

# Model coefficients
model_caret$finalModel

# Examine model prediction for each fold
model_caret$resample

# Look at standard deviation around the Mean Square Error value by examining the Root Mean Square Error from each fold.
sd(model_caret$resample$RMSE^2)



# Let's look at the model using all data, no CV
full_model <- lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)
summary(full_model)

# Full data had an Rsquared=0.736
```

_f) In a paragraph, summarize your findings to address the primary question posed by the investigator (that has limited statistical knowledge)._



_a) Fit a model regressing rental rates (monthly) on all the other variables. Summarize your findings focusing on the significance of the predictors and the overall fit._
```{r problem_2a)}
## loading the data
com_df <- read_csv("./CommercialProperties.csv") %>% 
  janitor::clean_names() 

## rental rates regressed by other variables
full_lm <- lm(rental_rate ~ age + taxes + vacancy_rate + sq_footage, data = com_df)
summary(full_lm)
anova(full_lm)

# SLR
summary(lm(rental_rate ~ age, data = com_df))
summary(lm(rental_rate ~ taxes, data = com_df))
summary(lm(rental_rate ~ vacancy_rate, data = com_df))
summary(lm(rental_rate ~ sq_footage, data = com_df))
```

The vacancy rate is not a significant predictor due to its p-value is above 0.5.

_b) From this point forward, let us consider only the significant predictors. Create scatter plots for each of the predictors and the outcome ‘rental rates’. Comment on the form of these relationships._
```{r problem_2b)}
com_df %>% 
  pivot_longer(age:sq_footage,
               names_to = "predictor",
               values_to = "value") %>% 
  ggplot(aes(x = value, y = rental_rate, color = predictor)) +
  geom_point() + facet_wrap(~predictor, scales = "free", nrow = 2, ncol = 2,
                            labeller = labeller(predictor = 
                                                  c(`age` = "Age", 
                                                    `sq_footage` = "Sqrt(footage)",
                                                    `taxes` = "Taxes", 
                                                    `vacancy_rate` = "Vacancy rate"))) +
  labs(x = "Values",
       y = "Rental rate",
       color = "Predictor") +
  theme(strip.text.x = element_text(size = 12, face = "bold")) +
  scale_color_discrete(labels = c("Age", "Sqrt(footage)", "Taxes", "Vacancy rate"))
```
According to the scatter plots,

_c) Fit a multiple linear regression containing only the significant predictors._ 
```{r problem_2c)}
sig_ml <- lm(rental_rate ~ age + taxes + sq_footage, data = com_df)
broom::tidy(sig_ml)
```

_d) Use the model from part 3, keep all the significant predictors in their initial form, but let us play with the ‘age of the property’.1) Given the relationship between ‘age’ and ‘rental rates’ (part 2), would you use higher order terms to fit ‘age’? Should you use centered ‘age’? _
```{r problem_2d1)}
age_lm <- lm(rental_rate ~ age, data = com_df)
summary(age_lm)

## residuals vs covariate(age)
com_df %>% 
  dplyr::select(rental_rate, age) %>% 
  modelr::add_residuals(age_lm) %>% 
  modelr::add_predictions(age_lm) %>% 
  ggplot(aes(x = age, y = resid)) + geom_point() +
  geom_hline(yintercept = 0, size = 1.2, color = "red") +
  geom_smooth(se = FALSE, span = .4)


```

I would introduce square or higher-order transformation in the model building. Even though the "residual vs covariate(age)" is evenly distributed around 0, the distribution of points are not even across the x direction. The two ends of x axis have points spreading across the y axis but have a few points in the middle. 
centered age?

_d) 2) Or maybe you want to keep it ‘linear’, and try ‘segments and knots’ instead? If using piecewise linear, comment on your choice of knot(s)._
```{r problem_2d2)}

## try different values as knots(7, 15)
knot_df <- com_df %>% 
  mutate(spline_5 = (age - 5) * (age >= 5),
         spline_6 = (age - 6) * (age >= 6),
         spline_7 = (age - 7) * (age >= 7),
         spline_8 = (age - 8) * (age >= 8))


```

_d) 3) Try one or both approaches and recommend a model._
```{r problem_2d3)}
## build high-ordered model and test it
high_order_df <- com_df %>% 
  mutate(sq_age = age^2)

high_sig_ml <- lm(rental_rate ~ age + sq_age + taxes + sq_footage, data = high_order_df)
summary(high_sig_ml)


## linear model with knots
knot5_lm <- lm(rental_rate ~ taxes + age + spline_5 + sq_footage, data = knot_df)
summary(knot5_lm)
knot6_lm <- lm(rental_rate ~ taxes + age + spline_6 + sq_footage, data = knot_df)
summary(knot6_lm)
knot7_lm <- lm(rental_rate ~ taxes + spline_7 + sq_footage, data = knot_df)
summary(knot7_lm)
knot8_lm <- lm(rental_rate ~ taxes + spline_8 + sq_footage, data = knot_df)
summary(knot8_lm)
knot9_lm <- lm(rental_rate ~ taxes + spline_9 + sq_footage, data = knot_df)
summary(knot9_lm)

knot_df %>% 
  nest(spline_5:spline_9, )


```

_e) Test whether the recommended model from part 4 is ‘superior’ to the model in part 3. Write and comment on the ‘final’ model that you chose._
```{r problem_2d3)}
summary(sig_ml)

```

